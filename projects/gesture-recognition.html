<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="description" content="NeuroCalc: Dual Hands Arithmetic Recognition — Antoine Besson" />
  <title>NeuroCalc | Antoine Besson</title>
  <link rel="stylesheet" href="../style.css" />
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet" />
</head>
<body>

  <header class="navbar" id="navbar">
    <div class="navbar__container">
      <a href="../index.html" class="navbar__logo">&larr; Back</a>
      <div class="navbar__actions">
        <button class="theme-toggle" id="themeToggle" aria-label="Toggle dark mode">
          <svg class="icon icon--sun" xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24"
               fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/>
            <line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/>
            <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/>
            <line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/>
            <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/>
          </svg>
          <svg class="icon icon--moon" xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24"
               fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <path d="M21 12.79A9 9 0 1 1 11.21 3a7 7 0 0 0 9.79 9.79z"/>
          </svg>
        </button>
      </div>
    </div>
  </header>

  <main class="project-detail">
    <div class="project-detail__container">

      <div class="project-detail__tags">
        <span class="tag">PyTorch 2.0</span>
        <span class="tag">MediaPipe</span>
        <span class="tag">ST-GCN</span>
      </div>

      <h1 class="project-detail__title">NeuroCalc: Dual Hands Arithmetic Recognition</h1>

      <p class="project-detail__intro">
        A computer vision system capable of recognizing dynamic arithmetic operations performed by human hands in real-time, achieving 93.65% validation accuracy using Spatio-Temporal Graph Convolutional Networks on MediaPipe hand landmarks.
      </p>

      <figure class="project-detail__figure">
        <img src="assets/neuro_calc_exemple.png" alt="NeuroCalc live inference example — recognising hand gestures as arithmetic operations" />
      </figure>

      <section class="project-detail__section">
        <h2>Abstract</h2>
        <p>
          NeuroCalc is a real-time hand gesture recognition system that interprets arithmetic operations (digits 0–9 and operators +, −, ×, ÷, =) performed with both hands in front of a webcam. The hands are modelled as a dynamic graph of 42 vertices extracted by MediaPipe, projected into a canonical rotation-invariant reference frame, and classified by a Spatio-Temporal Graph Convolutional Network (ST-GCN). The system reaches 93.65% validation accuracy on a custom 15-class dataset and runs at 30 FPS on CPU thanks to an asynchronous strided inference engine that reduces compute load by approximately 75%.
        </p>
      </section>

      <section class="project-detail__section">
        <h2>Motivation</h2>
        <p>
          This project is my second time tackling an image/video ML model. In my previous project, I used a standard CNN with a public dataset to recognise characters on a licence plate. Standard Convolutional Neural Networks struggle with dynamic hand gestures due to high variance in camera distance and orientation. I wanted to explore a fundamentally different approach: treating the hands as a graph rather than a grid of pixels.
        </p>
        <p>
          By projecting the data into a canonical reference frame before it reaches the neural network, I drastically reduced the complexity of the learning manifold. I also built my own dataset from scratch — 21 videos per character across 15 classes (digits and operators) — to have full control over the training distribution and to practise the end-to-end ML pipeline.
        </p>
      </section>

      <section class="project-detail__section">
        <h2>Implementation</h2>
        <h3>Geometric Kernel — Unified Scene Projection</h3>
        <p>
          Raw coordinates are sensitive to the user's position, so the Geometric Kernel anchors the coordinate system to the dominant hand's wrist using Gram-Schmidt orthogonalisation. This canonicalises the dual-hand topology into a rotation-invariant manifold, effectively decoupling the semantic gesture from the user's global orientation.
        </p>
        <h3>Adaptive Graph Convolution</h3>
        <p>
          To capture semantic relationships between unconnected joints, the network uses an Adaptive Graph Convolution layer that can "invent" new edges beyond the physical skeleton. This lets the model learn cross-hand relationships (e.g., both index fingers touching for "=").
        </p>
        <h3>Temporal Convolution Blocks</h3>
        <p>
          Temporal evolution is handled by interleaved TCN blocks with dilated convolutions. These give the network a wide receptive field over time, which helps distinguish between static digits and dynamic signs like "Minus" (a sweeping motion).
        </p>
        <h3>Asynchronous Strided Inference</h3>
        <p>
          To maintain a smooth 30 FPS UI, the inference engine runs the ST-GCN on every N-th frame in a background thread, reducing CPU load by approximately 75% while the main thread continues rendering the webcam feed and overlay.
        </p>
      </section>

      <section class="project-detail__section">
        <h2>How to Use It</h2>
        <ol>
          <li>Clone the repository and install dependencies:<br><code>cd neuro_calc<br>poetry install</code></li>
          <li>Record your own hand gesture data (or skip to step 4 to use the pre-trained model):<br><code>poetry run python tools/recorder.py</code><br><em>Use SPACE to record, N/P to switch classes, Q to quit.</em></li>
          <li>Train the model:<br><code>poetry run python train.py</code></li>
          <li>Run live inference:<br><code>poetry run python inference.py</code></li>
        </ol>
      </section>

    </div>
  </main>

  <footer class="footer">
    <div class="footer__container">
      <p class="footer__copy">&copy; 2026 Antoine Besson</p>
    </div>
  </footer>

  <script src="../script.js"></script>
</body>
</html>